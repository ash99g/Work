{
 "cells": [
  {
   "cell_type": "raw",
   "id": "deeea7d1-3ea1-44b2-8a5f-aa2e2f395dff",
   "metadata": {},
   "source": [
    "Q1. Explain the difference between linear regression and logistic regression models. Provide an example of\n",
    "    a scenario where logistic regression would be more appropriate.\n",
    "Ans. Linear Regression is used to handle regression problems whereas Logistic regression is used to handle the classification problems.              Linear regression provides a continuous output but Logistic regression provides discreet output.\n",
    "     Linear regression is a data analysis technique that predicts the value of unknown data by using another related and known data value. It        mathematically models the unknown or dependent variable and the known or independent variable as a linear equation.\n",
    "     Logistic regression is a data analysis technique that uses mathematics to find the relationships between two data factors. It then uses        this relationship to predict the value of one of those factors based on the other\n",
    "     Example-Credit scoring"
   ]
  },
  {
   "cell_type": "raw",
   "id": "68361b06-397a-478c-998f-06b8dfb1697f",
   "metadata": {},
   "source": [
    "Q2. What is the cost function used in logistic regression, and how is it optimized?\n",
    "Ans.The cost function used in Logistic Regression is Log Loss.Logarithmic loss indicates how close a prediction probability comes to the           actual/corresponding true value. As a result, it turns out that maximizing the likelihood is equivalent to minimizing the mean square error     (MSE), i.e. this error function is widely used in regression problems for a reason."
   ]
  },
  {
   "cell_type": "raw",
   "id": "be364aa3-0555-44cd-8502-db85333fbf3d",
   "metadata": {},
   "source": [
    "Q3. Explain the concept of regularization in logistic regression and how it helps prevent overfitting.\n",
    "Ans.Regularization is a way of finding a good bias-variance tradeoff by tuning the complexity of the model. It is a very useful method to           handle collinearity (high correlation among features), filter out noise from data, and eventually prevent overfitting.\n",
    "\n",
    "    That's the set of parameters. In short, Regularization in machine learning is the process of regularizing the parameters that constrain,       regularizes, or shrinks the coefficient estimates towards zero. In other words, this technique discourages learning a more complex or           flexible model, avoiding the risk of Overfitting."
   ]
  },
  {
   "cell_type": "raw",
   "id": "8c6c4af6-4306-4075-9331-7e3ef59a82be",
   "metadata": {},
   "source": [
    "Q4. What is the ROC curve, and how is it used to evaluate the performance of the logistic regression model?\n",
    "Ans.An ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification       thresholds. This curve plots two parameters: True Positive Rate. False Positive Rate.\n",
    "\n",
    "    ROC curves in logistic regression are used for determining the best cutoff value for predicting whether a new observation is a \"failure\"       (0) or a \"success\" (1)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d6b4525e-a384-4637-9251-94b1e21da7e7",
   "metadata": {},
   "source": [
    "Q5. What are some common techniques for feature selection in logistic regression? How do these\n",
    "    techniques help improve the model's performance?\n",
    "Ans.1.Extra Tree Classifier.\n",
    "    2.Pearson correlation.\n",
    "    3.Forward selection.\n",
    "    4.Chi-square.\n",
    "    5.Logit (Logistic Regression model)    \n",
    "    \n",
    "    Feature Selection is the method of reducing the input variable to your model by using only relevant data and getting rid of noise in data.     It is the process of automatically choosing relevant features for your machine learning model."
   ]
  },
  {
   "cell_type": "raw",
   "id": "bb064ba9-5293-4780-93c3-fdb8f3a00f3c",
   "metadata": {},
   "source": [
    "Q6. How can you handle imbalanced datasets in logistic regression? What are some strategies for dealing\n",
    "    with class imbalance?\n",
    "Ans.In logistic regression, another technique comes handy to work with imbalance distribution. This is to use class-weights in accordance with     the class distribution. Class-weights is the extent to which the algorithm is punished for any wrong prediction of that class.\n",
    "\n",
    "    1.Sample Dataset\n",
    "    2.Random Under-Sampling. \n",
    "    3.Random Over-Sampling.\n",
    "    4.Random Under-Sampling With Imblearn. \n",
    "    5.Random Over-Sampling With imblearn.\n",
    "    6.Under-Sampling: Tomek Links.\n",
    "    7.Synthetic Minority Oversampling Technique (SMOTE)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d9346079-c621-47fe-88be-85a8cc67b5c5",
   "metadata": {},
   "source": [
    "Q7. Can you discuss some common issues and challenges that may arise when implementing logistic\n",
    "    regression, and how they can be addressed? For example, what can be done if there is multicollinearity\n",
    "    among the independent variables?\n",
    "Ans.1.If the number of observations is lesser than the number of features, Logistic Regression should not be used, otherwise, it may lead to         overfitting.\n",
    "    2.It constructs linear boundaries.\n",
    "\t3.The major limitation of Logistic Regression is the assumption of linearity between the dependent variable and the independent variables.\n",
    "    4.It can only be used to predict discrete functions. Hence, the dependent variable of Logistic Regression is bound to the discrete number         set.\n",
    "    \n",
    "    Logistic regression can also be prone to overfitting, particularly when there is a high number of predictor variables within the model.         Regularization is typically used to penalize parameters large coefficients when the model suffers from high dimensionality.\n",
    "    \n",
    "    To reduce the amount of multicollinearity found in a statistical model, one can remove the specific variables identified as the most           collinear. You can also try to combine or transform the offending variables to lower their correlation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
