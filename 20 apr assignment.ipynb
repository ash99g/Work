{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b54eedc-5c66-4e7b-9339-9e9fecff3d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What is the KNN algorithm?\n",
    "# Ans.K-Nearest Neighbour is one of the simplest Machine Learning algorithms based on Supervised Learning technique.\n",
    "#     K-NN algorithm assumes the similarity between the new case/data and available cases and put the new case into the category that \n",
    "#     is most similar to the available categories.\n",
    "#     K-NN algorithm stores all the available data and classifies a new data point based on the similarity. This means when new data appears\n",
    "#     then it can be easily classified into a well suite category by using K- NN algorithm.\n",
    "#     K-NN algorithm can be used for Regression as well as for Classification but mostly it is used for the Classification problems.\n",
    "#     K-NN is a non-parametric algorithm, which means it does not make any assumption on underlying data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f4ee8d-16be-42ca-a7fb-f239fdfb7f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. How do you choose the value of K in KNN?\n",
    "# Ans.The choice of k will largely depend on the input data as data with more outliers or noise will likely perform better with higher values\n",
    "#     of k. Overall, it is recommended to have an odd number for k to avoid ties in classification, and cross-validation tactics can \n",
    "#     help you choose the optimal k for your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78394a1a-c932-4269-905b-cf153f4b953f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. What is the difference between KNN classifier and KNN regressor?\n",
    "# Ans.The key differences are: \n",
    "#     KNN regression tries to predict the value of the output variable by using a local average. \n",
    "#     KNN classification attempts to predict the class to which the output variable belong by computing the local probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45cbccd-cf82-47ef-866f-9c95d4ec324a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. How do you measure the performance of KNN?\n",
    "# Ans.The k-nearest neighbour classification (k-NN) is one of the most popular distance-based algorithms. This classification is based \n",
    "#     on measuring the distances between the test sample and the training samples to determine the final classification output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7257944b-f10f-40d3-a8de-b3b36adcd6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. What is the curse of dimensionality in KNN?\n",
    "# Ans.The “Curse of Dimensionality” is a tongue in cheek way of stating that there's a ton of space in high-dimensional data sets.\n",
    "#     The size of the data space grows exponentially with the number of dimensions. This means that the size of your data set must also \n",
    "#     grow exponentially in order to keep the same density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612d0d28-5d3c-4e70-ab11-e6ef76c9138c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. How do you handle missing values in KNN?\n",
    "# Ans.The idea in kNN methods is to identify 'k' samples in the dataset that are similar or close in the space. Then we use these 'k'\n",
    "#     samples to estimate the value of the missing data points. Each sample's missing values are imputed using the mean value of \n",
    "#     the 'k'-neighbors found in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e555da9-eb10-4331-80f5-0bd0bb88aec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?\n",
    "# Ans.KNN regression tries to predict the value of the output variable by using a local average.\n",
    "#     regression model: codomain of model is a continuous space, e.g. R\n",
    "#     Knn Regression: Predicts a value by using the mean of the k nearest neighbors.\n",
    "\n",
    "#     KNN classification attempts to predict the class to which the output variable belong by computing the local probability.\n",
    "#     classification model: codomain of model is a discrete space, e.g. {0,1}.\n",
    "#     Knn Classifier: Predicts a class by using the highest majority category among its k nearest neighbors.\n",
    "\n",
    "#     KNN classifier is better for classification type of problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d78206-9337-49e7-b5c0-a365ab600c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?\n",
    "# Ans.Here are some of the advantages of using the k-nearest neighbors algorithm:\n",
    "\n",
    "#     It's easy to understand and simple to implement\n",
    "#     It can be used for both classification and regression problems\n",
    "#     It's ideal for non-linear data since there's no assumption about underlying data\n",
    "#     It can naturally handle multi-class cases\n",
    "#     It can perform well with enough representative data\n",
    "\n",
    "#    Here are some of the disadvantages of using the k-nearest neighbors algorithm:\n",
    "\n",
    "#     Associated computation cost is high as it stores all the training data\n",
    "#     Requires high memory storage\n",
    "#     Need to determine the value of K\n",
    "#     Prediction is slow if the value of N is high\n",
    "#     Sensitive to irrelevant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ed8c2f-b308-47b6-b264-4f1290977c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "# Ans.Euclidean distance is the shortest path between source and destination which is a straight line.  \n",
    "#     Manhattan distance is sum of all the real distances between source(s) and destination(d) and each distance are always the straight lines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dfb597-8556-49ee-9a69-27998ff476df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q10. What is the role of feature scaling in KNN?\n",
    "# Ans.Yes, feature scaling is required to get the better performance of the KNN algorithm. For Example, Imagine a dataset having n number of\n",
    "#     instances and N number of features. There is one feature having values ranging between 0 and 1. Meanwhile, there is also a feature \n",
    "#     that varies from -999 to 999."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
