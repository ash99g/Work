{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd55430f-62ab-4a09-812a-dda4490b9048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how can they be mitigated?\n",
    "\n",
    "# Ans.1.Overfitting is an undesirable machine learning behavior that occurs when the machine learning model gives accurate predictions for training data but not for new data.\n",
    "#     2.Underfitting is a scenario in data science where a data model is unable to capture the relationship between the input and output variables accurately, generating a high error rate on both the training set and unseen data.\n",
    "\n",
    "#     1.When the model memorizes the noise and fits too closely to the training set, the model becomes “overfitted,” and it is unable to generalize well to new data.\n",
    "#     2.when a model is underfitted, it cannot establish the dominant trend within the data, resulting in training errors and poor performance of the model. \n",
    "\n",
    "#     1.The algorithms you use include by default regularization parameters meant to prevent overfitting.\n",
    "#     2.Using a more complex model, for instance by switching from a linear to a non-linear model or by adding hidden layers to your neural network, will very often help solve underfitting. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09024278-aba1-4775-aeac-a5000e387c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2: How can we reduce overfitting? Explain in brief.\n",
    "\n",
    "# Ans.You can prevent overfitting by diversifying and scaling your training data set or using some other data science strategies, like those given below.\n",
    "\n",
    "#     1.Early stopping\n",
    "#     Early stopping pauses the training phase before the machine learning model learns the noise in the data. However, getting the timing right is important; else the model will still not give accurate results.\n",
    "\n",
    "#     2.Pruning\n",
    "#     You might identify several features or parameters that impact the final prediction when you build a model. Feature selection—or pruning—identifies the most important features within the training set and eliminates irrelevant ones.  \n",
    "\n",
    "#     3.Regularization\n",
    "#     Regularization is a collection of training/optimization techniques that seek to reduce overfitting. These methods try to eliminate those factors that do not impact the prediction outcomes by grading features based on importance. \n",
    "\n",
    "#     4.Ensembling\n",
    "#     Ensembling combines predictions from several separate machine learning algorithms. Some models are called weak learners because their results are often inaccurate. Ensemble methods combine all the weak learners to get more accurate results. \n",
    "\n",
    "#     5.Data augmentation\n",
    "#     Data augmentation is a machine learning technique that changes the sample data slightly every time the model processes it. You can do this by changing the input data in small ways. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511cfada-88fd-4106-9f22-f1f4a4a703d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3: Explain underfitting. List scenarios where underfitting can occur in ML.\n",
    "\n",
    "# Ans.Underfitting is a scenario in data science where a data model is unable to capture the relationship between the input and output variables accurately, generating a high error rate on both the training set and unseen data.\n",
    "\n",
    "#     Underfitting occurs when a model is too simple — informed by too few features or regularized too much — which makes it inflexible in learning from the dataset. Simple learners tend to have less variance in their predictions but more bias towards wrong outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3fc972-8c89-4647-a18b-657837123f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and variance, and how do they affect model performance?\n",
    "\n",
    "# Ans.In statistics and machine learning, the bias–variance tradeoff is the property of a model that the variance of the parameter estimated across samples can be reduced by increasing the bias in the estimated parameters.\n",
    "\n",
    "#     Bias and variance are inversely connected. It is impossible to have an ML model with a low bias and a low variance. When a data engineer modifies the ML algorithm to better fit a given data set, it will lead to low bias—but it will increase variance.\n",
    "\n",
    "#     A model with high variance may represent the data set accurately but could lead to overfitting to noisy or otherwise unrepresentative training data. In comparison, a model with high bias may underfit the training data due to a simpler model that overlooks regularities in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6859ab-15cb-4334-a50b-e26e42375955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models.\n",
    "#     How can you determine whether your model is overfitting or underfitting?\n",
    "\n",
    "# Ans.1.Overfitting can be identified by checking validation metrics such as accuracy and loss. The validation metrics usually increase until a point where they stagnate or start declining when the model is affected by overfitting.\n",
    "\n",
    "#     2.We can determine whether a predictive model is underfitting or overfitting the training data by looking at the prediction error on the training data and the evaluation data. Your model is underfitting the training data when the model performs poorly on the training data.\n",
    "\n",
    "#     We can determine whether a predictive model is underfitting or overfitting the training data by looking at the prediction error on the training data and the evaluation data. \n",
    "#     Your model is underfitting the training data when the model performs poorly on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384852b5-4def-4e49-9ce4-f7a82ba5634e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias\n",
    "#     and high variance models, and how do they differ in terms of their performance?\n",
    "\n",
    "# Ans.Bias is a phenomenon that occurs in the machine learning model wherein you have used an algorithm and it does not fit properly. This means that’s the function used here is of little relevance to the scenario and it’s not able to extract the correct patterns. \n",
    "#     Variance, on the other hand, specifies the amount of variation that the estimate of the target function will change if different training data was used. It says about how much a random variable deviates from its expected value.\n",
    "\n",
    "#     Some examples of high bias and high variance models:\n",
    "#      Linear Regression, Linear Discriminant Analysis and Logistic Regression.\n",
    "\n",
    "#     A model with high variance may represent the data set accurately but could lead to overfitting to noisy or otherwise unrepresentative training data. In comparison, a model with high bias may underfit the training data due to a simpler model that overlooks regularities in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34a6143-5c13-49c7-afc2-b1b452688289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe\n",
    "#     some common regularization techniques and how they work.\n",
    "\n",
    "# Ans. 1.Regularization refers to techniques that are used to calibrate machine learning models in order to minimize the adjusted loss function and prevent overfitting or underfitting.\n",
    "\n",
    "#      2.Regularization in machine learning is the process of regularizing the parameters that constrain, regularizes, or shrinks the coefficient estimates towards zero. \n",
    "#        In other words, this technique discourages learning a more complex or flexible model, avoiding the risk of Overfitting.\n",
    "\n",
    "#      3.Regularization is used in machine learning as a solution to overfitting by reducing the variance of the ML model under consideration. \n",
    "#        Regularization can be implemented in multiple ways by either modifying the loss function, sampling method, or the training approach itself."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
