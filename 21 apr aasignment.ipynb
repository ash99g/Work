{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6e7ace-2c01-45a8-aa3f-eeef8f05e80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance metric in KNN? \n",
    "#     How might this difference affect the performance of a KNN classifier or regressor?\n",
    "# Ans.Euclidean Distance represents the shortest distance between two vectors.It is the square root of the sum of squares of differences \n",
    "#     between corresponding elements. The Euclidean distance metric corresponds to the L2-norm of a difference between vectors and vector spaces.\n",
    "#     Manhattan distance is a metric in which the distance between two points is the sum of the absolute differences of their Cartesian coordinates. \n",
    "#     In a simple way of saying it is the total sum of the difference between the x-coordinates and y-coordinates.\n",
    "\n",
    "#     Evaluating classification performance for KNN works the same as evaluating performance for any other classification algorithm we need a \n",
    "#     set of predictions, and the corresponding ground-truth labels for each of the points you made a prediction on. You can then compute \n",
    "#     evaluation metrics such as Precision, Recall, Accuracy and F1-Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed39575f-e775-43aa-bd38-e5c2b0b160a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be used to determine the optimal k value?\n",
    "# Ans.The choice of k will largely depend on the input data as data with more outliers or noise will likely perform better with higher values\n",
    "#     of k. Overall, it is recommended to have an odd number for k to avoid ties in classification, and cross-validation tactics can help you\n",
    "#     choose the optimal k for your dataset.\n",
    "\n",
    "#    Perform K-means clustering with all these different values of K. For each of the K values, we calculate average distances to the centroid across all data points.\n",
    "#    Plot these points and find the point where the average distance from the centroid falls suddenly (“Elbow”)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63453e1-e41a-474d-bdb5-db87fb3bd3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor?\n",
    "#     In what situations might you choose one distance metric over the other?\n",
    "# Ans.We mean by the 'best distance metric' (in this review) is the one that allows the KNN to classify test examples with the highest\n",
    "#     precision, recall and accuracy, i.e. the one that gives best performance of the KNN in terms of accuracy.\n",
    "\n",
    "#     Distance metrics are used in supervised and unsupervised learning to calculate similarity in data points. They improve the performance,\n",
    "#     whether that's for classification tasks or clustering. \n",
    "#       There are a few conditions that the distance metric must satisfy:\n",
    "#       Non-negativity: d(x, y) >= 0.\n",
    "#       Identity: d(x, y) = 0 if and only if x == y.\n",
    "#       Symmetry: d(x, y) = d(y, x)\n",
    "#       Triangle Inequality: d(x, y) + d(y, z) >= d(x, z)\n",
    "\n",
    "#     Euclidean distance is one of the most used distance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf130365-8f7b-499f-b36a-b301fc502578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect the performance of the model? \n",
    "#     How might you go about tuning these hyperparameters to improve model performance?\n",
    "# Ans.Some examples of Hyperparameters \n",
    "#     The k in kNN or K-Nearest Neighbour algorithm.\n",
    "#     Learning rate for training a neural network.\n",
    "#     Train-test split ratio.\n",
    "#     Batch Size.\n",
    "#     Number of Epochs.\n",
    "#     Branches in Decision Tree.\n",
    "#     Number of clusters in Clustering Algorithm.\n",
    "\n",
    "#    A hyperparameter controls the learning process and therefore their values directly impact other parameters of the model such as weights \n",
    "#    and biases which consequently impacts how well our model performs. The accuracy of any machine learning model is most often improved\n",
    "#    by fine-tuning these hyperparameters.\n",
    "\n",
    "#  Steps to Perform Hyperparameter Tuning\n",
    "#    Select the right type of model.\n",
    "#    Review the list of parameters of the model and build the HP space.\n",
    "#    Finding the methods for searching the hyperparameter space.\n",
    "#    Applying the cross-validation scheme approach.\n",
    "#    Assess the model score to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc557a91-05a5-45cb-bd33-5cbb624d34b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What\n",
    "#     techniques can be used to optimize the size of the training set?\n",
    "# Ans.So if dataset is large, there will be a lot of processing which may adversely impact the performance of the algorithm. \n",
    "#     KNN is also very sensitive to noise in the dataset. If the dataset is large, there are chances of noise in the dataset which \n",
    "#     adversely affect the performance of KNN algorithm.\n",
    "\n",
    "#     The most popular iterative method for solving the optimization problems in machine learning is the Gradient Descent Algorithm and \n",
    "#     its variants, Stochastic Gradient Descent and the MiniBatch Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7b2f5e-507d-4ef4-8217-ca9970fec8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you\n",
    "#     overcome these drawbacks to improve the performance of the model?\n",
    "# Ans.Some Disadvantages of KNN\n",
    "#     Accuracy depends on the quality of the data.\n",
    "#     With large data, the prediction stage might be slow.\n",
    "#     Sensitive to the scale of the data and irrelevant features.\n",
    "#     Require high memory – need to store all of the training data.\n",
    "#     Given that it stores all of the training, it can be computationally expensive.\n",
    "\n",
    "#    KNN is sensitive to outliers and missing values and hence we first need to impute the missing values and get rid of the outliers\n",
    "#    before applying the KNN algorithm.\n",
    "#    The product of the validity and distance weight for each data point produces a weighted training dataset. This reduces a multi-dimensional\n",
    "#    dataset into one- dimensional dataset, which improves the efficiency of kNN."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
